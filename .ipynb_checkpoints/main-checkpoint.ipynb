{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5615f400-a4bf-46ec-99eb-3e7b3f1126cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scraper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m sys.path.append(os.getcwd())\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscraper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scrape_nirf_data\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfuzzywuzzy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfuzzywuzzy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scraper'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nbimporter\n",
    "import os\n",
    "import pandas as pd\n",
    "from scraper import scrape_nirf_data\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from merge_parameter_scores import merge_csvs\n",
    "\n",
    "years = list(range(2017, 2026))\n",
    "all_data = []\n",
    "institutes_2018_not_in_others = []\n",
    "institutes_2017_not_in_others = []\n",
    "\n",
    "os.makedirs(\"csv_data\", exist_ok=True)\n",
    "\n",
    "print(\"\\nProcessing and mapping data for 2017...\")\n",
    "df_2017 = scrape_nirf_data(2017)\n",
    "mapped_df_2017 = None\n",
    "if df_2017 is not None:\n",
    "    if \"Institute ID\" not in df_2017.columns:\n",
    "        print(\"Skipping 2017: 'Institute ID' column missing\")\n",
    "    else:\n",
    "        base_cols_2017 = [\"Institute ID\", \"Name\"]\n",
    "        metric_cols = [\"Rank\", \"Score\", \"TLR (100)\", \"RPC (100)\", \"GO (100)\", \"OI (100)\", \"PERCEPTION (100)\"]\n",
    "        existing_metrics_2017 = [col for col in metric_cols if col in df_2017.columns]\n",
    "        df_2017_processed = df_2017[base_cols_2017 + existing_metrics_2017].copy()\n",
    "        rename_map_2017 = {col: f\"{col}_{2017}\" for col in existing_metrics_2017}\n",
    "        df_2017_processed = df_2017_processed.rename(columns=rename_map_2017)\n",
    "\n",
    "        institute_names_later_years = {}\n",
    "        for year in range(2018, 2026):  \n",
    "            temp_df = scrape_nirf_data(year)\n",
    "            if temp_df is not None and \"Name\" in temp_df.columns and \"Institute ID\" in temp_df.columns:\n",
    "                name_id_map = temp_df[[\"Name\", \"Institute ID\"]].drop_duplicates(subset=[\"Name\"]).set_index(\"Name\")[\"Institute ID\"].to_dict()\n",
    "                institute_names_later_years.update(name_id_map)\n",
    "\n",
    "        mapped_ids_2017 = {}\n",
    "        unmapped_2017 = []\n",
    "        for index, row in df_2017_processed.iterrows():\n",
    "            best_match, score = process.extractOne(row[\"Name\"], institute_names_later_years.keys(), scorer=fuzz.ratio)\n",
    "            if best_match and score > 85:\n",
    "                mapped_ids_2017[row[\"Institute ID\"]] = institute_names_later_years[best_match]\n",
    "            else:\n",
    "                unmapped_2017.append(row.to_dict())\n",
    "\n",
    "        df_2017_processed[\"Consistent Institute ID\"] = df_2017_processed[\"Institute ID\"].map(mapped_ids_2017).fillna(df_2017_processed[\"Institute ID\"])\n",
    "        mapped_df_2017 = df_2017_processed.drop(columns=[\"Institute ID\"], errors=\"ignore\").rename(columns={\"Consistent Institute ID\": \"Institute ID\"})\n",
    "        mapped_df_2017.to_csv(\"csv_data/nirf_data_2017.csv\", index=False)\n",
    "        print(\"Saved and mapped: nirf_data_2017.\")\n",
    "        if unmapped_2017:\n",
    "            institutes_2017_not_in_others = pd.DataFrame(unmapped_2017)\n",
    "        else:\n",
    "            institutes_2017_not_in_others = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(\"Could not retrieve 2017 data.\")\n",
    "\n",
    "\n",
    "print(\"\\nProcessing and mapping data for 2018...\")\n",
    "df_2018 = scrape_nirf_data(2018)\n",
    "mapped_df_2018 = None\n",
    "if df_2018 is not None:\n",
    "    if \"Institute ID\" not in df_2018.columns:\n",
    "        print(\"Skipping 2018: 'Institute ID' column missing\")\n",
    "    else:\n",
    "        base_cols_2018 = [\"Institute ID\", \"Name\"]\n",
    "        metric_cols = [\"Rank\", \"Score\", \"TLR (100)\", \"RPC (100)\", \"GO (100)\", \"OI (100)\", \"PERCEPTION (100)\"]\n",
    "        existing_metrics_2018 = [col for col in metric_cols if col in df_2018.columns]\n",
    "        df_2018_processed = df_2018[base_cols_2018 + existing_metrics_2018].copy()\n",
    "        rename_map_2018 = {col: f\"{col}_{2018}\" for col in existing_metrics_2018}\n",
    "        df_2018_processed = df_2018_processed.rename(columns=rename_map_2018)\n",
    "\n",
    "        institute_names_later_years = {}\n",
    "        for year in range(2019, 2026):\n",
    "            temp_df = scrape_nirf_data(year)\n",
    "            if temp_df is not None and \"Name\" in temp_df.columns and \"Institute ID\" in temp_df.columns:\n",
    "                name_id_map = temp_df[[\"Name\", \"Institute ID\"]].drop_duplicates(subset=[\"Name\"]).set_index(\"Name\")[\"Institute ID\"].to_dict()\n",
    "                institute_names_later_years.update(name_id_map)\n",
    "\n",
    "        mapped_ids_2018 = {}\n",
    "        unmapped_2018 = []\n",
    "        for index, row in df_2018_processed.iterrows():\n",
    "            best_match, score = process.extractOne(row[\"Name\"], institute_names_later_years.keys(), scorer=fuzz.ratio)\n",
    "            if best_match and score > 85:\n",
    "                mapped_ids_2018[row[\"Institute ID\"]] = institute_names_later_years[best_match]\n",
    "            else:\n",
    "                unmapped_2018.append(row.to_dict())\n",
    "\n",
    "        df_2018_processed[\"Consistent Institute ID\"] = df_2018_processed[\"Institute ID\"].map(mapped_ids_2018).fillna(df_2018_processed[\"Institute ID\"])\n",
    "        mapped_df_2018 = df_2018_processed.drop(columns=[\"Institute ID\"], errors=\"ignore\").rename(columns={\"Consistent Institute ID\": \"Institute ID\"})\n",
    "        mapped_df_2018.to_csv(\"csv_data/nirf_data_2018.csv\", index=False)\n",
    "        print(\"Saved and mapped: nirf_data_2018.\")\n",
    "        if unmapped_2018:\n",
    "            institutes_2018_not_in_others = pd.DataFrame(unmapped_2018)\n",
    "        else:\n",
    "            institutes_2018_not_in_others = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(\"Could not retrieve 2018 data.\")\n",
    "\n",
    "\n",
    "for year in range(2019, 2026):\n",
    "    print(f\"\\nProcessing data for {year}...\")\n",
    "    try:\n",
    "        csv_path = f\"csv_data/nirf_data_{year}.csv\"\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"üìÅ Cached file found: {csv_path}\")\n",
    "            df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            df = scrape_nirf_data(year)\n",
    "            if df is not None:\n",
    "                if \"Institute ID\" not in df.columns:\n",
    "                    print(f\"Skipping {year}: 'Institute ID' column missing\")\n",
    "                    continue\n",
    "\n",
    "                base_cols = [\"Institute ID\", \"Name\"]\n",
    "                metric_cols = [\"Rank\", \"Score\", \"TLR (100)\", \"RPC (100)\", \"GO (100)\", \"OI (100)\", \"PERCEPTION (100)\"]\n",
    "                existing_metrics = [col for col in metric_cols if col in df.columns]\n",
    "                df_processed = df[base_cols + existing_metrics].copy()\n",
    "                rename_map = {col: f\"{col}_{year}\" for col in existing_metrics}\n",
    "                df_processed = df_processed.rename(columns=rename_map)\n",
    "                all_data.append(df_processed)\n",
    "                df_processed.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved: nirf_data_{year}.\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "\n",
    "        if df is not None and \"Institute ID\" in df.columns:\n",
    "            base_cols = [\"Institute ID\", \"Name\"]\n",
    "            metric_cols = [\"Rank\", \"Score\", \"TLR (100)\", \"RPC (100)\", \"GO (100)\", \"OI (100)\", \"PERCEPTION (100)\"]\n",
    "            existing_metrics = [col for col in metric_cols if f\"{col}_{year}\" in df.columns or col in df.columns]\n",
    "\n",
    "            needs_rename = any(col in df.columns for col in metric_cols)\n",
    "            if needs_rename:\n",
    "                df = df[base_cols + existing_metrics].copy()\n",
    "                rename_map = {col: f\"{col}_{year}\" for col in existing_metrics}\n",
    "                df = df.rename(columns=rename_map)\n",
    "\n",
    "            all_data.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "if all_data and mapped_df_2017 is not None:\n",
    "    combined_df = mapped_df_2017.copy()  \n",
    "    all_data_to_merge = [mapped_df_2018] + all_data  \n",
    "\n",
    "    for df in all_data_to_merge:\n",
    "        if 'Institute ID' in combined_df.columns and 'Institute ID' in df.columns:\n",
    "            combined_df = pd.merge(combined_df, df.drop(columns=[\"Name\"], errors=\"ignore\"), on=\"Institute ID\", how=\"outer\", suffixes=('', '_y'))\n",
    "            combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]\n",
    "        elif 'Institute ID' in combined_df.columns:\n",
    "            combined_df = pd.merge(combined_df, df.drop(columns=[\"Name\"], errors=\"ignore\"), left_on=\"Institute ID\", right_on=\"Institute ID\", how=\"outer\")\n",
    "        elif 'Institute ID' in df.columns:\n",
    "            combined_df = pd.merge(combined_df, df.drop(columns=[\"Name\"], errors=\"ignore\"), left_on=\"Institute ID\", right_on=\"Institute ID\", how=\"outer\")\n",
    "        else:\n",
    "            print(\"Warning: 'Institute ID' column not found in one of the DataFrames during merge.\")\n",
    "\n",
    "    all_data_with_2017 = [mapped_df_2017] + [mapped_df_2018] + all_data\n",
    "    latest_names = {}\n",
    "    for df in reversed(all_data_with_2017):\n",
    "        if 'Institute ID' in df.columns and 'Name' in df.columns:\n",
    "            name_map = df.set_index(\"Institute ID\")[\"Name\"].dropna().to_dict()\n",
    "            latest_names.update(name_map)\n",
    "\n",
    "    combined_df[\"Name\"] = combined_df[\"Institute ID\"].map(latest_names)\n",
    "\n",
    "    fixed_cols = [\"Institute ID\", \"Name\"]\n",
    "    ordered_cols = []\n",
    "    metric_cols_all = [\"Rank\", \"Score\", \"TLR (100)\", \"RPC (100)\", \"GO (100)\", \"OI (100)\", \"PERCEPTION (100)\"]\n",
    "    for metric in metric_cols_all:\n",
    "        yearly_cols = sorted([col for col in combined_df.columns if col.startswith(metric + \"_\")])\n",
    "        ordered_cols.extend(yearly_cols)\n",
    "\n",
    "    remaining_cols = [col for col in combined_df.columns if col not in fixed_cols + ordered_cols]\n",
    "    combined_df = combined_df[fixed_cols + ordered_cols + remaining_cols]\n",
    "\n",
    "    cols = combined_df.columns.tolist()\n",
    "    if \"Name\" in cols:\n",
    "        cols.remove(\"Name\")\n",
    "        cols.insert(1, \"Name\")\n",
    "    combined_df = combined_df[cols]\n",
    "\n",
    "    combined_df.to_csv(\"csv_data/nirf_combined_data.csv\", index=False)\n",
    "    combined_df.to_json(\"nirf_combined_data.json\", orient=\"records\", indent=2)\n",
    "    print(\"\\nAll data saved to nirf_combined_data.\")\n",
    "    merge_csvs()\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data extracted for merging.\")\n",
    "\n",
    "\n",
    "print(\"\\nüîó Combining all years...\")\n",
    "csv_dir = os.path.join(\"csv_data\")\n",
    "all_years_data = []\n",
    "name_map_latest = {}\n",
    "\n",
    "for year in range(2017, 2026):\n",
    "    csv_file = os.path.join(csv_dir, f\"nirf_data_{year}.csv\")\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"‚ùå Missing: {csv_file}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    if \"Name\" in df.columns and \"Institute ID\" in df.columns:\n",
    "        latest_name_map = df.set_index(\"Institute ID\")[\"Name\"].dropna().to_dict()\n",
    "        name_map_latest.update(latest_name_map)\n",
    "\n",
    "    all_years_data.append(df)\n",
    "\n",
    "\n",
    "def merge_yearwise_data(dfs):\n",
    "    combined = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        df = df.drop(columns=[\"Name\"], errors=\"ignore\")\n",
    "        combined = pd.merge(combined, df, on=\"Institute ID\", how=\"outer\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "if all_years_data:\n",
    "    combined_df = merge_yearwise_data(all_years_data)\n",
    "    combined_df[\"Name\"] = combined_df[\"Institute ID\"].map(name_map_latest)\n",
    "\n",
    "    fixed_cols = [\"Institute ID\", \"Name\"]\n",
    "    rest_cols = [col for col in combined_df.columns if col not in fixed_cols]\n",
    "\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    col_groups = defaultdict(list)\n",
    "    for col in rest_cols:\n",
    "        match = re.match(r\"(.+)_\\d{4}$\", col)\n",
    "        if match:\n",
    "            key = match.group(1)\n",
    "            col_groups[key].append(col)\n",
    "        else:\n",
    "            col_groups[\"Other\"].append(col)\n",
    "\n",
    "    ordered_cols = []\n",
    "    for metric in [\"Rank\", \"Score\", \"TLR (100)\", \"RPC (100)\", \"GO (100)\", \"OI (100)\", \"PERCEPTION (100)\"]:\n",
    "        if metric in col_groups:\n",
    "            ordered_cols.extend(sorted(col_groups[metric]))\n",
    "            del col_groups[metric]\n",
    "\n",
    "    sample_param_path = None\n",
    "    for y in range(2025, 2018, -1):\n",
    "        output_folder = f\"output{y}\"\n",
    "        if os.path.isdir(output_folder):\n",
    "            for college in os.listdir(output_folder):\n",
    "                param_file = os.path.join(output_folder, college, \"parameter_scores.csv\")\n",
    "                if os.path.isfile(param_file):\n",
    "                    sample_param_path = param_file\n",
    "                    break\n",
    "        if sample_param_path:\n",
    "            break\n",
    "\n",
    "    if sample_param_path:\n",
    "        param_df = pd.read_csv(sample_param_path)\n",
    "        parameter_order = param_df.columns.tolist()\n",
    "    else:\n",
    "        parameter_order = []\n",
    "\n",
    "    for param in parameter_order:\n",
    "        if param in col_groups:\n",
    "            ordered_cols.extend(sorted(col_groups[param]))\n",
    "            del col_groups[param]\n",
    "\n",
    "    for param, cols in col_groups.items():\n",
    "        ordered_cols.extend(sorted(cols))\n",
    "\n",
    "    combined_df = combined_df[fixed_cols + ordered_cols]\n",
    "\n",
    "    combined_df.to_csv(os.path.join(csv_dir, \"nirf_combined_data.csv\"), index=False)\n",
    "    combined_df.to_json(\"nirf_combined_data.json\", orient=\"records\", indent=2)\n",
    "\n",
    "    print(\"‚úÖ Combined data saved to:\")\n",
    "    print(\"  ‚Üí csv_data/nirf_combined_data.csv\")\n",
    "    print(\"  ‚Üí nirf_combined_data.json\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No yearly data to combine.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117f3682-5b4e-4721-bfda-6b5fb84d7dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nbimporter in c:\\users\\amang\\appdata\\roaming\\python\\python312\\site-packages (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip  install nbimporter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
